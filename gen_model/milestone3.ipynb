{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modern NLP: Course project Milestone 3**\n",
    "\n",
    "#### **Team**: Alexander Sternfeld, Silvia Romanato and Antoine Bonnet (`syntax-sorcerers`)\n",
    "\n",
    "> ### **Project Description**\n",
    "> \n",
    "> **Remember**: In **Milestone 1**, we picked a robust prompting stategy to get accurate answers from ChatGPT, which we used to generate answers for questions from EPFL course content. The generated answers were then rated by human annotators. The data collected (available at `project_reference/interactions_v1.json` and `project_reference/solutions_v1.json`) will be used in Milestone 3 for the supervised fine-tuning of a language model to answer questions from EPFL course content.\n",
    ">\n",
    "> However, high-quality assistants such as ChatGPT are trained using more than only\n",
    "supervised learning. They use a technique called Reinforcement Learning with Human\n",
    "Feedback (RLHF). RLHF requires your training procedure to have access to a reward model\n",
    "that can evaluate multiple different responses and rank them according to their suitability. \n",
    ">\n",
    "> In **Milestone 2**, we successfully trained a classifier reward model with the [RoBERTa](https://arxiv.org/abs/1907.11692) Transformer-based model base on the EPFL and StackOverflow datasets to rate the quality of answers given a question. This model will now be used to train a **policy model** with RLHF to rank multiple answers from the same question.\n",
    ">\n",
    "> In **Milestone 3**:, we now fine-tune a generative pretrained language model so that it learns to produce better demonstrations when prompted with a question from your course. We train our model using supervised learning on some of the data we have collected in the first two parts of your project. We also use our reward function to evaluate the quality of the text generations produced by our model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abonnet/Desktop/NLPProject/project-m3-syntax-sorcerers/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# To run this notebook, you need to install the following packages:\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "from load_data import *\n",
    "from finetune import *\n",
    "from chatbot import *\n",
    "from gen_script_syntax_sorcerers import *\n",
    "\n",
    "import json\n",
    "\n",
    "os.environ[\"NO_DEPRECATION_WARNING\"] = \"true\"\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training the ChatBot**\n",
    "\n",
    "Our goal is to fine-tune a generative pre-trained language model using supervised learning on the collected data from milestone 1 and 2. This fine-tuning process helps the model learn to generate better responses specific to EPFL course content.\n",
    "\n",
    "In this notebook, we select the base model, pre-process the labelled fine-tuning data, then run the fine-tuning process. We also evaluate the performance of the fine-tuned model on a validation set.\n",
    "\n",
    "**REMOVE THIS**: Use your reward model to evaluate the quality of the generated text and guide the fine-tuning process. You can employ RLHF techniques to further improve the chatbot's performance if you choose to do so.\n",
    "\n",
    "**Requirements**: \n",
    "1. The generative model should be able to generate proper answers given a question. \n",
    "2. It does not have to handle multi-interaction prompts. You will only be evaluated on one turn prompts.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Model selection**\n",
    "\n",
    "We use [Google's Large-sized T5 multilingual model](https://huggingface.co/t5-large) as the base for our chat engine. We select its `large` version (770M parameters) because it provides a good balance between computational resources and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load base model from HuggingFace (takes a few minutes the first time)\n",
    "BASE_MODEL_NAME = 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cuda':\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    model.to(device)\n",
    "else: \n",
    "    print('Using CPU.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Dataset selection**\n",
    "\n",
    "To fine-tune our chatbot to answer EPFL course content, we use a high-quality combination of two question-answering datasets. The full dataset is available at `data/gen_model/gen_dataset_syntax-sorcerers.json`. We use the following sources to collect our training data:\n",
    "\n",
    "1. **StackOverflow dataset**: The [StackOverflow dataset](https://www.kaggle.com/datasets/stackoverflow/stackoverflow) contains 27M+ answers on a wide variety of forums from which we select 9 topics: computer science, computer science theory, data science, mathematics, physics, chemistry, engineering, software engineering, mechanics and quantum physics. We only keep answers that were accepted by the original poster with at least 2 upvotes. This dataset contains 139264 unique questions and their corresponding accepted answer. \n",
    "\n",
    "2. **EPFL Student Interactions dataset**: This dataset contains student interactions with ChatGPT concerning questions on EPFL course content. From this dataset, we select provided official solutions as well as question-answer pairs that were rated with the highest confidence level (5) by students, since using lower quality answers may introduce noise and lead to incorrect responses. We only select one-shot interactions. This dataset contains 4450 questions with 7412 distinct answers. \n",
    "\n",
    "\n",
    "**NOTE**: We train the chatbot with QA-pairs of the following form: Text: \"`Human: What is 2+2?\\n\\nAssistant: `\" --> Label: \"`4`\". This means that when we will have our trained model, we will prepend the \"`Human: `\" prefix and append the \"`\\n\\nAssistant: `\" suffix to the actual prompt. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed StackOverflow data.\n",
      "Loading pre-processed EPFL data.\n",
      "\n",
      "Number of unique questions in StackExchange dataset: 95041\n",
      "Number of unique questions in EPFL dataset: 4450\n",
      "Number of QA samples in StackExchange dataset: 95041\n",
      "Number of QA samples in EPFL dataset: 7872\n",
      "Total number of QA samples: 102913\n",
      "\n",
      "Combined dataset already exists at /Users/abonnet/Desktop/NLPProject/project-m3-syntax-sorcerers/data/gen_model/gen_dataset_syntax-sorcerers.json.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-processed data\n",
    "stack_df = load_stack_data()\n",
    "EPFL_df = load_EPFL_data()\n",
    "\n",
    "print(\"\\nNumber of unique questions in StackExchange dataset: {}\".format(len(stack_df['question_id'].unique())))\n",
    "print(\"Number of unique questions in EPFL dataset: {}\".format(len(EPFL_df['question_id'].unique())))\n",
    "print('Number of QA samples in StackExchange dataset: {}'.format(len(stack_df)))\n",
    "print('Number of QA samples in EPFL dataset: {}'.format(len(EPFL_df)))\n",
    "print('Total number of QA samples: {}'.format(len(stack_df) + len(EPFL_df)))\n",
    "\n",
    "# Save as a combined json file\n",
    "QA_PATH = os.path.join(DATA_DIR, 'gen_model', f'gen_dataset_{TEAM_NAME}.json')\n",
    "if os.path.exists(QA_PATH):\n",
    "    print(f'\\nCombined dataset already exists at {QA_PATH}.')\n",
    "if not os.path.exists(QA_PATH): \n",
    "    print(f'\\nSaving combined dataset to {QA_PATH}.')\n",
    "    merged_data = { \n",
    "        'EPFL': EPFL_df.to_dict(orient='records'),\n",
    "        'StackOverflow': stack_df.to_dict(orient='records')\n",
    "    }\n",
    "    with open(QA_PATH, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets into QADatasets, split train/val/test\n",
    "EPFL_dataset = load_dataset(EPFL_df, tokenizer, seed=SEED)\n",
    "stack_dataset = load_dataset(stack_df, tokenizer, seed=SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Training the Chatbot**\n",
    "\n",
    "We now fine-tune the GPT2 pre-trained language model using supervised learning over two round of fine-tuning. \n",
    "\n",
    "In the first round, the pre-trained model is fine-tuned on the larger and diverse StackOverflow dataset allows the model to learn general language patterns, syntax, and common knowledge across a wide range of topics. This pre-training helps the model acquire a strong language understanding foundation, which can later be further fine-tuned to the specific domain of EPFL course content.\n",
    "\n",
    "In the second round, the model is fine-tuned on the collected EPFL-specific dataset. We use maximum likelihood estimation (MLE) to train the model. The model can then focus on adapting to the specific question-answer patterns, terminology, and context of EPFL course content. This fine-tuning step enables the model to specialize in generating accurate and relevant responses for EPFL-specific queries.\n",
    "\n",
    "During training, we use maximum likelihood estimation (MLE) to train the model. We evaluate the current model using the validation set set with metrics including perplexity, BLEU and ROUGE scores. After each round of training, we evaluate the model on the test set. We use the validation set to monitor the model's performance during training and the test set to evaluate the final performance.\n",
    "\n",
    "We use the cross-entropy loss to fine-tune the GPT2 language model. It measures the dissimilarity between the model's predicted probability distribution over the vocabulary and the true distribution (labels). The model aims to minimize this loss during training to improve its generation capability. \n",
    "\n",
    "TODO: \n",
    "- Check metrics function: might add BERTScore\n",
    "- Which metric to choose to save model? Perplexity (aka loss) is not always a good metric to choose the best model.\n",
    "- Tuning hyperparameters: train batch size (8 vs 16 vs 32), learning rate (1e-4, 1e-5, 1e-6), num training epochs.\n",
    "- StackOverflow: lower num epochs (just learn general QA patterns), learning rate not sure (higher for pre-trained or finetuning on EPFL?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbots already fine-tuned. Skipping fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "# This runs the whole fine-tuning procedure\n",
    "finetune(seed=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Evaluation**\n",
    "\n",
    "> - Evaluate the performance of your chat engine using appropriate metrics, such as BLEU, ROUGE, or human evaluation.\n",
    "> - Continuously analyze and monitor the responses generated by the chat engine to identify areas for improvement. \n",
    "> - Collect user feedback and iterate on the model and training process accordingly.\n",
    "> - Experiment with different training techniques, architectures, and hyperparameters to optimize the chat engine's performance.\n",
    "> - Remember to iterate and experiment throughout the training process, as finding the optimal approach often requires testing different combinations of models, datasets, and training techniques. Regularly evaluate the performance of your chatbot, seek user feedback, and make adjustments as necessary to ensure its effectiveness as an educational assistant for EPFL course content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Preparing submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94cdfd24-de94-4072-a5aa-a3a9dc953d2b</td>\n",
       "      <td>Soit \\(f\\) une fonction paire (resp. impaire) ...</td>\n",
       "      <td>Soit \\(f\\) une fonction paire, possédant\\nun d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3826e1bd-05f7-4f80-a6ad-937af83dc3e4</td>\n",
       "      <td>The goal of the 4 following questions is to pr...</td>\n",
       "      <td>MapTrCons, ConsAppend, IH, MapTrCons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f6f7e3a3-b63a-4f13-abbe-a2b3f74c64e5</td>\n",
       "      <td>What is the general relation between the entan...</td>\n",
       "      <td>Entanglement is sufficient but not necessary f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b473eff-0fc8-4fb0-9d10-bc49d18e52e3</td>\n",
       "      <td>Cet exercice est un bref rappel de maths, il d...</td>\n",
       "      <td>1. f(x)=\\cos(x) &amp;\\Rightarrow F(x)=\\sin(x)+C\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6654c2b0-4dcd-432e-9c8e-26d638afe0ef</td>\n",
       "      <td>What is the worst case complexity of listing f...</td>\n",
       "      <td>['$O(number of direntries in the directory)$']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  \\\n",
       "0  94cdfd24-de94-4072-a5aa-a3a9dc953d2b   \n",
       "1  3826e1bd-05f7-4f80-a6ad-937af83dc3e4   \n",
       "2  f6f7e3a3-b63a-4f13-abbe-a2b3f74c64e5   \n",
       "3  0b473eff-0fc8-4fb0-9d10-bc49d18e52e3   \n",
       "4  6654c2b0-4dcd-432e-9c8e-26d638afe0ef   \n",
       "\n",
       "                                            question  \\\n",
       "0  Soit \\(f\\) une fonction paire (resp. impaire) ...   \n",
       "1  The goal of the 4 following questions is to pr...   \n",
       "2  What is the general relation between the entan...   \n",
       "3  Cet exercice est un bref rappel de maths, il d...   \n",
       "4  What is the worst case complexity of listing f...   \n",
       "\n",
       "                                              answer  \n",
       "0  Soit \\(f\\) une fonction paire, possédant\\nun d...  \n",
       "1               MapTrCons, ConsAppend, IH, MapTrCons  \n",
       "2  Entanglement is sufficient but not necessary f...  \n",
       "3   1. f(x)=\\cos(x) &\\Rightarrow F(x)=\\sin(x)+C\\n...  \n",
       "4     ['$O(number of direntries in the directory)$']  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read df from prompts path\n",
    "with open(PROMPTS_PATH, 'r') as f:\n",
    "    prompts = pd.read_json(f, encoding='utf-8')\n",
    "\n",
    "# Format question and answer\n",
    "prompts = prompts.replace({np.nan: None})\n",
    "prompts['question'] = prompts.apply(lambda x: Q_from_solutions(x['question'], x['choices']), axis=1)\n",
    "prompts['answer'] = prompts.apply(lambda x: A_from_solutions(x['answer'], x['explanation']), axis=1)\n",
    "\n",
    "# Remove columns choices, explanation\n",
    "prompts = prompts.drop(columns=['choices', 'explanation'])\n",
    "prompts.head()\n",
    "# Generate answers using chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
