{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Modern NLP: Course project - Milestone 2**\n",
        "\n",
        "#### **Team**: Alexander Sternfeld, Silvia Romanato and Antoine Bonnet (`syntax-sorcerers`)\n",
        "\n",
        "\n",
        "> **Remember**: In Milestone 1, we picked a robust prompting stategy to get accurate answers from ChatGPT, which we used to generate answers for questions from EPFL course content. The generated answers were then rated by human annotators. The data collected (available at `project_reference/interactions_v1.json` and `project_reference/solutions_v1.json`) will be used in Milestone 3 for the supervised fine-tuning of a language model to answer questions from EPFL course content.\n",
        "> \n",
        "> However, high-quality assistants such as ChatGPT are trained using more than only\n",
        "supervised learning. They use a technique called Reinforcement Learning with Human\n",
        "Feedback (RLHF). RLHF requires your training procedure to have access to a reward model\n",
        "that can evaluate multiple different responses and rank them according to their suitability. \n",
        ">\n",
        "> **For newcomers**: Make sure that you have run the `scripts/data_preparation.ipynb` notebook before running this notebook.\n",
        "\n",
        "## **Training a reward model**\n",
        "\n",
        "This notebook aims to trains a **reward model** to rate the quality of answers given a question. This model will later be used to train a **policy model** with RLHF to rank multiple answers from the same question.\n",
        "\n",
        "We will use the [RoBERTa](https://arxiv.org/abs/1907.11692) transformer-based model on the [StackOverflow](https://www.kaggle.com/datasets/stackoverflow/stackoverflow) datasets to predict the quality (and rank) multiple answers from the same question. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "16U476_fLvKf"
      },
      "outputs": [],
      "source": [
        "from reward_dataset import *\n",
        "from classifier_model import *\n",
        "#from regressive_model import *\n",
        "from evaluate import *\n",
        "from model import *\n",
        "\n",
        "STACK_PATH = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers_StackOverflow.json')\n",
        "EPFL_PATH = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers_EPFL.json')\n",
        "\n",
        "BASE_MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you are running this notebook on Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/MyDrive/Modern_NLP/Project/')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. **Loading training data**\n",
        "\n",
        "We first import the StackOverflow and EPFL data. These data was previously pre-processed and cleaned in the `scripts/data_preparation.ipynb` notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mKIZ6rIy9U2d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>chat</th>\n",
              "      <th>entry_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>Human: Can't use The SGD optimizer &lt;p&gt;I am usi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>Human: Can't use The SGD optimizer &lt;p&gt;I am usi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>Human: Preprocessing , EDA , and Feature Engin...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>Human: Preprocessing , EDA , and Feature Engin...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>Human: Examples of reversible computations &lt;p&gt;...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               chat  entry_id\n",
              "0  positive  Human: Can't use The SGD optimizer <p>I am usi...         0\n",
              "1  negative  Human: Can't use The SGD optimizer <p>I am usi...         1\n",
              "2  positive  Human: Preprocessing , EDA , and Feature Engin...         2\n",
              "3  negative  Human: Preprocessing , EDA , and Feature Engin...         3\n",
              "4  negative  Human: Examples of reversible computations <p>...         4"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the stackOverflow data\n",
        "stack_data = pd.read_json(STACK_PATH)\n",
        "stack_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chat</th>\n",
              "      <th>label</th>\n",
              "      <th>entry_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Human: Une conquille sphérique de rayon $R_1$ ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Human: Une conquille sphérique de rayon $R_1$ ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Human: Une conquille sphérique de rayon $R_1$ ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Human: Assume that we have a convolutional neu...</td>\n",
              "      <td>positive</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Human: Q: Which of the following functions rea...</td>\n",
              "      <td>negative</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                chat     label  entry_id\n",
              "0  Human: Une conquille sphérique de rayon $R_1$ ...  negative         0\n",
              "1  Human: Une conquille sphérique de rayon $R_1$ ...  negative         1\n",
              "2  Human: Une conquille sphérique de rayon $R_1$ ...  positive         2\n",
              "3  Human: Assume that we have a convolutional neu...  positive         3\n",
              "4  Human: Q: Which of the following functions rea...  negative         4"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the EPFL interaction data\n",
        "epfl_data = pd.read_json(EPFL_PATH, orient='records')\n",
        "epfl_data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. **Pre-trained base model**\n",
        "\n",
        "[RoBERTa](https://arxiv.org/abs/1907.11692) (**R**obustly **o**ptimized **B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is an upgraded version of the original [BERT](https://arxiv.org/abs/1810.04805) model released by Google in 2018. \n",
        "\n",
        "The model architecture consists of a **pre-trained tokenizer** (mapping text to vectors) and a pre-trained **Transformer-based model** (mapping vectors to vectors). \n",
        "\n",
        "We will use the **BERTbase** model, which is composed of a stack of 12 identical layers (number of Transformer blocks), each with 12 attention heads (size of a transformer block). The model is trained on a masked language modeling (MLM) objective, which means that the model is trained to predict randomly masked tokens in a sequence.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.researchgate.net/publication/352642553/figure/fig2/AS:1037416861282304@1624350862022/The-RoBERTa-model-architecture.ppm\"/>\n",
        "</p>\n",
        "\n",
        "We use the [HuggingFace](https://huggingface.co/) library to load the pre-trained model and tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Do not print warnings\n",
        "RoBERTa_base = AutoModel.from_pretrained(BASE_MODEL_NAME)\n",
        "RoBERTa_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. **Data pre-processing**  \n",
        "\n",
        "Following recommendations from the [InstructGPT paper](https://arxiv.org/pdf/2203.02155.pdf), we train the model with one batch per question. Each batch will therefore consist of $K$ question-answer pairs, where $K$ is the number of different answers for that question. \n",
        "\n",
        "We define a custom PyTorch `Dataset` class to that effect. We tokenize the data using the RoBERTa pre-trained tokenizer. We also **truncate** the tokenized question-answer pairs to 512 tokens (maximum length of a sequence that can be processed by RoBERTa). We found that too many question-answer pairs exceeded this limit, so we decided to truncate the text to the first 512 tokens. We also **pad** the text to 512 tokens if it is shorter than 512 tokens.\n",
        "\n",
        "We split the dataset into a training, validation and test `DataLoaders` using a 60/20/20 split with shuffling. Note that we keep answers to the same questions in the same set to avoid any data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = create_dataset(stack_data, RoBERTa_tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. **Classification model**\n",
        "\n",
        "We treat the task as a **binary classification** problem and add a **classification head** on top of the pre-trained model to predict whether any given question-answer pair has a correct answer or not. As previously, we pass batches of answers to the same question to the model, and the model will learn to rank the answers according to their quality.\n",
        "\n",
        "The difference now is that we will use a **binary cross-entropy loss** function to train the model. The model will be trained to predict the probability that a given answer is correct.\n",
        "\n",
        "We however need to account for class imbalances (we always have a single correct answer, but between 1 and 30 incorrect answers) which might lead to the model predicting that all answers are incorrect. To account for this, we use **focal loss**, which is a modified version of the binary cross-entropy loss function that down-weighs the loss of correct answers. This is equivalent to weighting the loss of incorrect answers by the ratio of incorrect answers to correct answers in the batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaForSequenceClassification(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_NAME, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL_NAME, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "# Replace the last layer with layers of width [768, 256, 32, 2]\n",
        "#model.classifier = nn.Sequential(nn.Linear(768, 256),nn.ReLU(),nn.Linear(256, 32),nn.ReLU(),nn.Linear(32, 2))\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = stack_data\n",
        "dataset = create_dataset(df, tokenizer)\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(  \n",
        "    output_dir=RUNS_DIR,            \n",
        "    evaluation_strategy=\"epoch\",        # Run evaluation every epoch\n",
        "    save_strategy=\"epoch\",              # Save checkpoint every epoch\n",
        "    num_train_epochs=1,                 # Number of training epochs\n",
        "    per_device_train_batch_size=16,     # Number of QA per batch (default=8)\n",
        "    per_device_eval_batch_size=16,      # Number of QA per batch (default=8)         \n",
        "    load_best_model_at_end=True,        # Load the best model when finished training \n",
        "    metric_for_best_model=\"accuracy\",   # Use accuracy to evaluate the best model\n",
        "    logging_strategy=\"steps\",           # Log val metrics every (logging_steps) batches\n",
        "    logging_steps=100,                  \n",
        "    logging_dir=LOGS_DIR,               \n",
        "    disable_tqdm=False, \n",
        "    report_to='all', \n",
        "    seed=1,\n",
        "    learning_rate=1e-5,                 # Learning rate\n",
        "    #gradient_accumulation_steps=10     # Accumulate 10 steps before backward pass\n",
        "    #weight_decay=0.01,                 # L2 regularization strength \n",
        "    #eval_accumulation_steps=10,         # Accumulate 10 steps before eval loss\n",
        ")\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['val'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model \n",
        "# Note: best model is automatically loaded at end of training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on test set and save results\n",
        "results = trainer.evaluate(dataset['test'])\n",
        "print(results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. **Evaluating the reward model**\n",
        "\n",
        "We now use the trained reward model to produce scores for the answers to the questions in test StackOverFlow dataset. We then compare the scores to the ground truth labels to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model from checkpoint\n",
        "checkpoint_path = os.path.join(RUNS_DIR, 'checkpoint')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "# Load the datasets\n",
        "datasets = [load_dataset(dataset_name, tokenizer) for dataset_name in DATASET_NAMES]\n",
        "\n",
        "# Evaluate the model on test sets and save results\n",
        "TRAINING_ARGS.run_name = 'evaluation'\n",
        "TRAINING_ARGS.output_dir = EVAL_DIR\n",
        "TRAINING_ARGS.report_to = None\n",
        "\n",
        "for i in range(len(datasets)):\n",
        "    print(f'\\nEvaluating on dataset [{i+1}/{len(datasets)}] ({DATASET_NAMES[i]}).')\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=TRAINING_ARGS,\n",
        "        train_dataset=datasets[i]['train'],\n",
        "        eval_dataset=datasets[i]['val'],\n",
        "        compute_metrics=compute_metrics,       \n",
        "    )\n",
        "    eval_res = trainer.evaluate(datasets[i]['test'])\n",
        "    print('Evaluation result:\\n', eval_res)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Preparing submission**\n",
        "\n",
        "We start by combining our datasets into a single json file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine both datasets into one\n",
        "data_path = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers.json')\n",
        "stack_path = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers_StackOverflow.json')\n",
        "epfl_path = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers_EPFL.json')\n",
        "StackOverflow_df = pd.read_json(stack_path, orient='records')\n",
        "EPFL_df = pd.read_json(epfl_path, orient='records')\n",
        "df = pd.concat([StackOverflow_df, EPFL_df], ignore_index=True)\n",
        "df.to_json(data_path, orient='records', indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trained model is saved to the `checkpoint` folder. We save the config files to the `reward_model` folder to be used by `evaluate.py`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from saved model path\n",
        "checkpoint_dir = os.path.join(BASE_DIR, 'checkpoint')\n",
        "data_path = os.path.join(BASE_DIR, 'data', 'reward_model', 'm2_reward_dataset_syntax-sorcerers.json')\n",
        "submission_folder = os.path.join(BASE_DIR, 'reward_model')\n",
        "hf_pretrained_model_name = \"roberta-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type roberta to instantiate a model of type ClassifierRewardModel. This is not supported for all configurations of models and can yield errors.\n"
          ]
        }
      ],
      "source": [
        "# Save model and model config to model_path directory\n",
        "model_config = ClassifierRewardModelConfig.from_pretrained(hf_pretrained_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_pretrained_model_name)\n",
        "model_config.problem_type = 'single_label_classification'\n",
        "model = ClassifierRewardModel(model_config)\n",
        "\n",
        "tokenizer.save_pretrained(submission_folder)\n",
        "model.save_pretrained(submission_folder)\n",
        "model_config.save_pretrained(submission_folder)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
